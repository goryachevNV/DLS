{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 .Домашнее задание. Линейные модели\n",
    "\n",
    "В этом задании вы напишете логистическую регрессию и логистическую регрессию с регуляризацией\n",
    "\n",
    "## 6.1 Домашнее задание. Линейные модели и методы оптимизации\n",
    "\n",
    "В этом домашнем задании вам необходимо прорешать ноутбук (https://colab.research.google.com/drive/1IwHPr6TrqAcRvLoMfcBf0X_eNVIBbd8y, скаченный файл: **[homework]linear_models_fall_2021.ipynb**) о линейных моделях и градиентном спуске. Сдача задания происходит по шагам, на которых необходимо вписывать реализацию написанных вами функций (контекст будет понятен из ноутбука) в форму сдачи Stepik-а. Мы рекомендуем сначала решать ноутбук, а потом уже тестировать в системе, так как в самом ноутбуке есть много подсказок.\n",
    "\n",
    "Ссылка на папку с материалами (https://drive.google.com/drive/folders/1OsmkJXvZHb7_afU1U8juY5Wok29dRkFp, скаченные файлы: **[homework]linear_models_fall_2021.ipynb**, **train.csv**)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Заполните пропуски в функциях grad_f и grad_descent_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    \"\"\"\n",
    "    :param x: np.array(np.float) размерности 2\n",
    "    :return: float\n",
    "    \"\"\"\n",
    "    return np.sum(np.sin(x)**2, axis=0)\n",
    "\n",
    "\n",
    "def grad_f(x): \n",
    "    \"\"\"\n",
    "    Градиент функциии f, определенной выше.\n",
    "    :param x: np.array[2]: float вектор длины 2\n",
    "    :return: np.array[2]: float вектор длины 2\n",
    "    \"\"\"\n",
    "    return np.sin(2.0*x) \n",
    "\n",
    "\n",
    "def grad_descent_2d(f, grad_f, lr, num_iter=100, x0=None):\n",
    "    \"\"\"\n",
    "    функция, которая реализует градиентный спуск в минимум для функции f от двух переменных. \n",
    "        :param f: скалярная функция двух переменных\n",
    "        :param grad_f: функция, возвращающая градиент функции f (устроена как реализованная вами выше grad_f)\n",
    "        :param lr: learning rate алгоритма\n",
    "        :param num_iter: количество итераций градиентного спуска\n",
    "        :return: np.array[num_iter, 2] пар вида (x, f(x))\n",
    "    \"\"\"\n",
    "    if x0 is None:\n",
    "        x0 = np.random.random(2)\n",
    "\n",
    "    # будем сохранять значения аргументов и значений функции \n",
    "    # в процессе град. спуска в переменную history\n",
    "    history = []\n",
    "\n",
    "    # итерация цикла -- шаг градиентнго спуска\n",
    "    curr_x = x0.copy()\n",
    "    for iter_num in range(num_iter):\n",
    "        entry = np.hstack((curr_x, f(curr_x)))\n",
    "        history.append(entry)\n",
    "        # не забудьте про lr!\n",
    "        curr_x -= lr * grad_f(curr_x)\n",
    "\n",
    "    return np.vstack(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Заполните пропуски в функции generate_batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batches(X, y, batch_size):\n",
    "    \"\"\"\n",
    "    param X: np.array[n_objects, n_features] --- матрица объекты-признаки\n",
    "    param y: np.array[n_objects] --- вектор целевых переменных\n",
    "    \"\"\"\n",
    "    assert len(X) == len(y)\n",
    "    np.random.seed(42)\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "    perm = np.random.permutation(len(X))\n",
    "\n",
    "    for batch_start in range(0, len(X), batch_size):\n",
    "        if (batch_start+batch_size) > (len(X)-1): return \n",
    "        yield (X[perm[batch_start:batch_start+batch_size]], y[perm[batch_start:batch_start+batch_size]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.4 Реализуйте методы fit и get_grad класса MyLogisticRegression.\n",
    "\n",
    "Напоминаем формулы:\n",
    "\n",
    "$$Loss(y, p) = -\\sum_{i=1}^{l} (y_i \\log (p_i) + (1 - y_i) \\log (1 - p_i))$$\n",
    "\n",
    "\n",
    "$$ \\frac{\\partial L}{\\partial w} = X^T (p - y)$$\n",
    "\n",
    "Функцию generate_batches, которую нужно использовать внутри .fit(),  мы уже реализовали за вас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit(x, w):\n",
    "    return np.dot(x, w)\n",
    "\n",
    "def sigmoid(h):\n",
    "    return 1. / (1 + np.exp(-h))\n",
    "\n",
    "\n",
    "class MyLogisticRegression(object):\n",
    "    def __init__(self):\n",
    "        self.w = None\n",
    "\n",
    "    def fit(self, X, y, epochs=10, lr=0.1, batch_size=100):\n",
    "        n, k = X.shape        \n",
    "        if self.w is None:\n",
    "            np.random.seed(42)\n",
    "            # Вектор столбец в качестве весов\n",
    "            self.w = np.random.randn(k + 1)\n",
    "\n",
    "        X_train = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        # Положите в лист losses лосс на каждом батче. Не нужно усреднять лосс по эпохе.\n",
    "\n",
    "        for i in range(epochs):\n",
    "            for X_batch, y_batch in generate_batches(X_train, y, batch_size):\n",
    "                #В X_train уже добавлен вектор 1\n",
    "\n",
    "                predictions = self._predict_proba_internal(X_batch)\n",
    "                loss = self.__loss(y_batch, predictions) \n",
    "\n",
    "                assert (np.array(loss).shape == tuple()), \"Лосс должен быть скаляром!\" \n",
    "\n",
    "                losses.append(loss)\n",
    "                self.w -= lr * self.get_grad(X_batch, y_batch, predictions)\n",
    "\n",
    "        return losses\n",
    "\n",
    "    def get_grad(self, X_batch, y_batch, predictions):\n",
    "        \"\"\"\n",
    "\n",
    "        param X_batch: np.array[batch_size, n_features + 1] --- матрица объекты-признаки\n",
    "        param y_batch: np.array[batch_size] --- батч целевых переменных\n",
    "        param predictions: np.array[batch_size] --- батч вероятностей классов\n",
    "\n",
    "        Принимает на вход X_batch с уже добавленной колонкой единиц. \n",
    "        Выдаёт градиент функции потерь в логистической регрессии\n",
    "        как сумму градиентов функции потерь на всех объектах батча\n",
    "        ВНИМАНИЕ! Нулевая координата вектора весов -- это BIAS, а не вес признака. \n",
    "        Также не нужно ДЕЛИТЬ ГРАДИЕНТ НА РАЗМЕР БАТЧА:\n",
    "        нас интересует не среднее, а сумма. \n",
    "        В качестве оператора умножения матриц можно использовать @ \n",
    "\n",
    "        Выход -- вектор-столбец градиентов для каждого веса (np.array[n_features + 1])\n",
    "        \"\"\"\n",
    "\n",
    "        #компонент градиента из логрегрессии \n",
    "        #следите за размерностями\n",
    "\n",
    "        grad_basic = X_batch.T @ (predictions - y_batch)\n",
    "        assert grad_basic.shape == (X_batch.shape[1],) , \"Градиенты должны быть столбцом из k_features + 1 элементов\"\n",
    "\n",
    "        return grad_basic\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        n, k = X.shape\n",
    "        X_ = np.concatenate((np.ones((n, 1)), X), axis=1)\n",
    "        return sigmoid(logit(X_, self.w))\n",
    "\n",
    "    def _predict_proba_internal(self, X): \n",
    "        \"\"\"\n",
    "        Возможно, вы захотите использовать эту функцию вместо predict_proba, поскольку\n",
    "        predict_proba конкатенирует вход с вектором из единиц, что не всегда удобно\n",
    "        для внутренней логики вашей программы\n",
    "        \"\"\"\n",
    "        return sigmoid(logit(X, self.w))\n",
    "\n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return self.predict_proba(X) >= threshold\n",
    "\n",
    "    def get_weights(self):\n",
    "        return self.w.copy() \n",
    "        # copy тут используется неспроста. Если copy не использовать, то get_weights()\n",
    "        # выдаст ссылку на объект, а, значит, модифицируя результат применения функции\n",
    "        # get_weights(), вы модифицируете и веса self.w. Если вы хотите модифицировать веса, \n",
    "        # (например, в fit), используйте self.w\n",
    "\n",
    "    def __loss(self, y, p):  \n",
    "        p = np.clip(p, 1e-10, 1 - 1e-10)\n",
    "        return -np.sum(y * np.log(p) + (1 - y) * np.log(1 - p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.5 Реализуйте класс логистической регрессии с обеими регуляризациями и оптимизацией с помощью SGD.\n",
    "\n",
    "Обратите внимание, что реализация ElasticNet отличается от реализации LogisticRegression только функцией потерь для оптимизации. Поэтому единственная функция, которая будет отличаться у двух методов, это self.get_grad.\n",
    "\n",
    "Поэтому в данном случае естественно применить паттерн наследования. Весь синтаксис наследования мы прописали за вас. Единственное, что вам осталось сделать, это переопределить метод get_grad в отнаследованном классе MyElasticLogisticRegression.\n",
    "\n",
    "Формулы:\n",
    "\n",
    "\n",
    "$$L_1 (w) = \\alpha \\sum_{j=1}^{n}|w_j| $$\n",
    "\n",
    "$$L_2 (w) = \\beta\\sum_{j=1}^{n}w_j^2$$\n",
    "\n",
    "$$\\frac{\\partial L_1}{\\partial w_1} = \\alpha \\cdot \\mathrm{sign}(w_1)$$\n",
    "\n",
    "$$ \\frac{\\partial L_2}{\\partial w_1} = 2\\beta w_1$$\n",
    "\n",
    "\n",
    "Класс MyLogisticRegression и функцию generate_batches в этом задании мы реализовали за вас."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyElasticLogisticRegression(MyLogisticRegression):\n",
    "    def __init__(self, l1_coef, l2_coef):\n",
    "        self.l1_coef = l1_coef\n",
    "        self.l2_coef = l2_coef\n",
    "        self.w = None\n",
    "\n",
    "    def get_grad(self, X_batch, y_batch, predictions):\n",
    "        \"\"\"\n",
    "        Принимает на вход X_batch с уже добавленной колонкой единиц. \n",
    "        Выдаёт градиент функции потерь в логистической регрессии с регуляризаторами\n",
    "        как сумму градиентов функции потерь на всех объектах батча + регуляризационное слагаемое\n",
    "        ВНИМАНИЕ! Нулевая координата вектора весов -- это BIAS, а не вес признака. \n",
    "        Bias в регуляризационные слагаемые не входит. Также не нужно ДЕЛИТЬ ГРАДИЕНТ НА РАЗМЕР БАТЧА:\n",
    "        нас интересует не среднее, а сумма. \n",
    "\n",
    "        Выход -- вектор-столбец градиентов для каждого веса (np.array[n_features + 1])\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        #YOUR CODE: компонент градиента из логрегрессии. Обнулять bias-компоненту этой составляющей градиента не нужно!\n",
    "        grad_basic = X_batch.T @ (predictions - y_batch)\n",
    "\n",
    "        #YOUR CODE: компонент градиента из l_1-регуляризации (не забудьте обнулить компоненту с bias)\n",
    "        grad_l1 = self.l1_coef * np.sign(self.w)\n",
    "        \n",
    "        #YOUR CODE: компонент градиента из l_2-регуляризации (не забудьте обнулить компоненту с bias)\n",
    "        grad_l2 = self.l2_coef * 2 * self.w\n",
    "        \n",
    "        grad_l1[0], grad_l2[0] = 0.0, 0.0\n",
    "\n",
    "\n",
    "        assert grad_l1[0] == grad_l2[0] == 0, \"Bias в регуляризационные слагаемые не входит!\"\n",
    "        assert grad_basic.shape == grad_l1.shape == grad_l2.shape == (X_batch.shape[1],) , \"Градиенты должны быть столбцом из k_features + 1 элементов\"\n",
    "\n",
    "        return grad_basic + grad_l1 + grad_l2"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
